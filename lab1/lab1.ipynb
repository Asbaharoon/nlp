{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(PATH):\n",
    "    import os\n",
    "    all_data = []\n",
    "    for path, dirs, files in os.walk(PATH):\n",
    "        temp = []\n",
    "        for filename in files:\n",
    "            filePath = path + '/' + filename\n",
    "            f = open(filePath, 'r', encoding='utf-8')\n",
    "            temp.append(parse_article([g for g in f]))\n",
    "\n",
    "        if temp:\n",
    "            all_data.append(temp)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_article(f):\n",
    "    \"\"\"tokenize sentence and transform to lower and padding\"\"\"\n",
    "    ret = [s.lower() for s in f]\n",
    "    ret = [nltk.word_tokenize(s) for s in ret]\n",
    "    for i, x in enumerate(ret):\n",
    "        try:\n",
    "            float(x)\n",
    "            ret[i] = '<N>'\n",
    "        except:\n",
    "            pass\n",
    "    ret = [['<s>'] + s + ['</s>'] for s in ret]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kn_model():\n",
    "    \"\"\"TODO: Bigram() + Pcontination()\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_article():\n",
    "    \"\"\"TODO: get label that has highest scores\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_bigram(s):\n",
    "    for i in range(len(s) - 1):\n",
    "        yield (s[i], s[i + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load and preprocess training data\n",
      "start training\n",
      "start testing\n",
      "Accuracy:  0.6046852122986823\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(\"load and preprocess training data\")\n",
    "    train_PATH = './r_train/'\n",
    "    all_training_data = read_data(train_PATH)\n",
    "    \n",
    "    print('start training')\n",
    "    # build kn model\n",
    "    unigram = [Counter([word for art in articles for sent in art for word in sent]) for articles in all_training_data]\n",
    "    bigram = [Counter(gram for art in articles for sent in art for gram in gen_bigram(sent)) for articles in all_training_data]\n",
    "    l_bigram = [len(bg) for bg in bigram] # |{(w', w''): 0 < c(w', w'')}|\n",
    "    categories = len(all_training_data)\n",
    "    w0_bigram = [defaultdict(set) for i in range(categories)] # {w': 0 < c(w0, w')}\n",
    "    w1_bigram = [defaultdict(set) for i in range(categories)] # {w': 0 < c(w', w1)}\n",
    "    for cat, bg in enumerate(bigram):\n",
    "        for w0, w1 in bg:\n",
    "            w0_bigram[cat][w0].add(w1)\n",
    "            w1_bigram[cat][w1].add(w0)\n",
    "\n",
    "\n",
    "    print(\"start testing\")\n",
    "    test_PATH = './r_test'\n",
    "    all_testing_data = read_data(test_PATH)\n",
    "    test_data_len = sum(len(t) for t in all_testing_data)\n",
    "\n",
    "    d = .75\n",
    "    total_hits = [0] * categories\n",
    "    for cat, articles in enumerate(all_testing_data):\n",
    "        for art in articles:\n",
    "            log_prob = [0] * categories\n",
    "            for c in range(categories):\n",
    "                for sent in art:\n",
    "                    for word in gen_bigram(sent):\n",
    "                        w0, w1 = word\n",
    "                        c_w0 = unigram[c][w0]\n",
    "                        if c_w0:\n",
    "                            bigram_term = max((bigram[c][word] - d) / c_w0, 0)\n",
    "                            lambda_term = d / c_w0 * len(w0_bigram[c][w0])\n",
    "                            pconti_term = len(w1_bigram[c][w1]) / l_bigram[c]\n",
    "                            prob = bigram_term + lambda_term * pconti_term + 0.15\n",
    "                            log_prob[c] += prob\n",
    "                        # else -\n",
    "            pred_cat = log_prob.index(max(log_prob))\n",
    "            total_hits[cat] += (cat == pred_cat)\n",
    "\n",
    "    total_hit = sum(total_hits)\n",
    "    print('Accuracy: ', total_hit / test_data_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
