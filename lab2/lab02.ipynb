{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import wikipedia\n",
    "from collections import defaultdict\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import wordnet as wn\n",
    "from scipy.spatial.distance import cosine\n",
    "# nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './udn.word2vec.bin'\n",
    "model = KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The amount of target synsets : 54\n"
     ]
    }
   ],
   "source": [
    "target_synsets = []\n",
    "for line in open('lab02_input.txt'):\n",
    "    word, synset, offset = line.strip().split('\\t')\n",
    "    target_synsets.append((word, wn.synset_from_pos_and_offset('n', int(offset))))\n",
    "\n",
    "print(\"The amount of target synsets :\", len(target_synsets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnec = {}\n",
    "with open('wn.ec.noun.txt') as f:\n",
    "    for line in f:\n",
    "        word, sense, ch = line.split('\\t', 3)\n",
    "        res = [w for w in ch.split('|') if w in model]\n",
    "        if word in model:\n",
    "            res.append(word)\n",
    "        if res:\n",
    "            wnec[sense] = res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wiki Link EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiec = {}\n",
    "with open('ec.link.txt') as f:\n",
    "    for line in f:\n",
    "        _, word, ch = line.split('\\t', 3)\n",
    "        word.replace('_', ' ')\n",
    "        ch = ch[2:]\n",
    "        if ch in model:\n",
    "            wikiec[word] = model[ch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"lab02_output.tsv\",\"w+\")\n",
    "result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chengscott/course/10710/NLP/lab2/venv/lib/python3.6/site-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/chengscott/course/10710/NLP/lab2/venv/lib/python3.6/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7801557779312134\n",
      "0.6944633722305298\n",
      "0.9233576655387878\n",
      "0.8686987161636353\n",
      "0.30378568172454834\n",
      "0.5829584002494812\n",
      "0.6366953253746033\n",
      "0.6906927227973938\n",
      "0.6194887757301331\n",
      "0.7347137928009033\n",
      "0.9471968412399292\n",
      "0.5368343591690063\n",
      "0.5441094636917114\n",
      "1.0\n",
      "0.6883959770202637\n",
      "0.9753903746604919\n",
      "1.0\n",
      "0.980416476726532\n",
      "0.8759234547615051\n",
      "0.8759234547615051\n",
      "0.8759234547615051\n",
      "0.9173532724380493\n",
      "0.2547925114631653\n",
      "0.2547925114631653\n",
      "0.2547925114631653\n",
      "0.2547925114631653\n",
      "0.8186461925506592\n",
      "0.8186461925506592\n",
      "0.8186461925506592\n",
      "0.8186461925506592\n",
      "0.8186461925506592\n",
      "0.9156430959701538\n",
      "0.8181314468383789\n",
      "0.9940304160118103\n",
      "0.30323734879493713\n",
      "0.3981686532497406\n",
      "0.7702730894088745\n",
      "0.9581615924835205\n",
      "0.6506733298301697\n",
      "0.8989349007606506\n",
      "0.9080107808113098\n",
      "0.9080107808113098\n",
      "0.5159494280815125\n",
      "0.7146669030189514\n",
      "0.7146669030189514\n",
      "0.7146669030189514\n",
      "0.8175163865089417\n",
      "0.5118826627731323\n",
      "0.545118510723114\n",
      "0.4713805019855499\n",
      "0.6494073867797852\n",
      "0.43743735551834106\n",
      "0.7783246040344238\n",
      "0.5843098759651184\n"
     ]
    }
   ],
   "source": [
    "for word, synset in target_synsets:\n",
    "    # synset: hypernyms, lemmas, hyponyms\n",
    "    s_hyper = list(synset.closure(lambda s: s.hypernyms(), 2))\n",
    "    s_hypo = list(synset.closure(lambda s: s.hyponyms(), 2))\n",
    "    s_hy = s_hyper + s_hypo\n",
    "    s_lemma = synset.lemmas()\n",
    "    s_family = s_hy + s_lemma\n",
    "    # corresponding words\n",
    "    n_s = synset.name().split('.')[0]\n",
    "    n_hy = [s.name().split('.')[0] for s in s_hy]\n",
    "    n_lemma = [s.name().split('.')[-1] for s in s_lemma]\n",
    "    n_family = n_hy + n_lemma\n",
    "    # corresponding chinese\n",
    "    ec_lemma = list(n_lemma)\n",
    "    for n in ec_lemma:\n",
    "        ec_lemma.extend(wnec.get(n, []))\n",
    "    ec_family = list(n_family)\n",
    "    for n in ec_family:\n",
    "        ec_family.extend(wnec.get(n, []))\n",
    "    # w2v model\n",
    "    v_l = [model[n] for n in ec_lemma if n in model]\n",
    "    if v_l:\n",
    "        m_s = model[n_s] if n_s in model else sum(v_l) / len(v_l)\n",
    "        m_family = [n for n in ec_family if n in model]\n",
    "        v_family = [model[n] - m_s for n in m_family]\n",
    "        v_s = sum(v_family) / len(v_family) + m_s\n",
    "    # wiki candidate\n",
    "    candidate = []\n",
    "    for lemma in m_family:\n",
    "        cur = [lemma]\n",
    "        if '_' in lemma:\n",
    "            cur = wikipedia.search(lemma, 3)\n",
    "        for page in cur:\n",
    "            try:\n",
    "                wiki = wikipedia.page(page)\n",
    "                candidate.append(page)\n",
    "            except:\n",
    "                pass\n",
    "    # score by vector cosine similarity\n",
    "    scores = {}\n",
    "    for cd in candidate:\n",
    "        s = [1 - cosine(v_s, model[c]) if c in model else 0 for c in cd.lower().split()]\n",
    "        scores[cd] = sum(s) / len(s)\n",
    "    page = 'can not match to wiki'\n",
    "    if scores:\n",
    "        page, score = min(scores.items(), key=lambda k: k[1])\n",
    "        #if score >= 0.5:\n",
    "        page = wikipedia.page(page).url\n",
    "    print(score)\n",
    "    result.append('\\t'.join([word, str(synset), page]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.write('\\n'.join(result))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
